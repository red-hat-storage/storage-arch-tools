#
# Usage:
#       gdeploy -c <filename>.conf
#
# This does backend setup first and then create the volume using the
# setup bricks.
#
#


[hosts]
n0
n1
n2


# Common backend setup for 2 of the hosts.
[backend-setup]
devices=/dev/sda
vgs=rhgs_vg
pools=rhgs_thinpool
lvs=rhgs_lv
mountpoints=/rhgs/bricks
brick_dirs=/rhgs/bricks/rep01

#[disktype]
#RAID6

#[diskcount]
#10

#[stripesize]
#256

[tune-profile]
rhgs-sequential-io

# If backend-setup is different for each host
# [backend-setup:10.70.46.13]
# devices=sdb
# brick_dirs=/gluster/brick/brick1
#
# [backend-setup:10.70.46.17]
# devices=sda,sdb,sdc
# brick_dirs=/gluster/brick/brick{1,2,3}
#

#[peer]
#manage=probe

[volume]
action=create
volname=rep3
replica=yes
replica_count=3
force=yes


[clients1]
action=mount
volname=rep3
hosts=c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11
fstype=glusterfs
client_mount_points=/rhgs/client/rep3

[clients2:n0]
action=mount
volname=rep2
hosts=c0,c1,c2,c3
fstype=nfs
client_mount_points=/rhgs/client/rep2

[clients3:n1]
action=mount
volname=rep2
hosts=c4,c5,c6,c7
fstype=nfs
client_mount_points=/rhgs/client/rep2

[clients4:n2]
action=mount
volname=rep2
hosts=c8,c9,c10,c11
fstype=nfs
client_mount_points=/rhgs/client/rep2
