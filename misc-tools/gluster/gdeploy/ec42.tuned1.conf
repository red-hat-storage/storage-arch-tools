#
# Usage:
#       gdeploy -c <filename>.conf
#
# This does backend setup first and then create the volume using the
# setup bricks.
#
#


[hosts]
n0
n1
n2
n3
n4
n5


# Common backend setup for 2 of the hosts.
[backend-setup]
devices=/dev/sda
vgs=rhgs_vg
pools=rhgs_thinpool
lvs=rhgs_lv
mountpoints=/rhgs/bricks
brick_dirs=/rhgs/bricks/ec42

[disktype]
RAID6

[diskcount]
10

[stripesize]
256

[tune-profile]
rhgs-sequential-io

# If backend-setup is different for each host
# [backend-setup:10.70.46.13]
# devices=sdb
# brick_dirs=/gluster/brick/brick1
#
# [backend-setup:10.70.46.17]
# devices=sda,sdb,sdc
# brick_dirs=/gluster/brick/brick{1,2,3}
#

#[peer]
#manage=probe

[volume]
action=create
volname=ec42
disperse=yes
disperse_count=4
redundancy_count=2
force=yes


[clients1]
action=mount
volname=ec42
hosts=c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11
fstype=glusterfs
client_mount_points=/rhgs/client/ec42

#[clients2:n0]
#action=mount
#volname=ec42
#hosts=c0,c6
#fstype=nfs
#client_mount_points=/rhgs/nfs-client/ec42
#
#[clients3:n1]
#action=mount
#volname=ec42
#hosts=c1,c7
#fstype=nfs
#client_mount_points=/rhgs/nfs-client/ec42
#
#[clients4:n2]
#action=mount
#volname=ec42
#hosts=c2,c8
#fstype=nfs
#client_mount_points=/rhgs/nfs-client/ec42
#
#[clients5:n3]
#action=mount
#volname=ec42
#hosts=c3,c9
#fstype=nfs
#client_mount_points=/rhgs/nfs-client/ec42
#
#[clients6:n4]
#action=mount
#volname=ec42
#hosts=c4,c10
#fstype=nfs
#client_mount_points=/rhgs/nfs-client/ec42
#
#[clients7:n5]
#action=mount
#volname=ec42
#hosts=c5,c11
#fstype=nfs
#client_mount_points=/rhgs/nfs-client/ec42
